{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "c82b6a99-8308-5e2b-a051-e1fd37400105",
        "openai_ephemeral_user_id": "28fd7e5e-3696-5ae6-a585-47380b7944b7",
        "openai_subdivision1_iso_code": "US-WA"
      }
    },
    "noteable": {
      "last_transaction_id": "8b3e0f68-4175-4b83-a0f1-2c7d0aac0b53"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "ed6f6809-e951-45c4-8bd3-80b916c72a59",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5ebb0019-cf7b-4d1e-bb71-a592e76dfc74"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:02:23.008399+00:00",
          "start_time": "2023-06-21T17:02:22.475458+00:00"
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\nimport faker\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Initialize Faker library\nfake = faker.Faker()\n\n# Define function to generate mock data\ndef generate_mock_data(n=1000):\n    data = []\n    for _ in range(n):\n        # Generate fake sentence\n        sentence = fake.sentence()\n        # Randomly assign a conversational technique\n        technique = random.choice(['affirmation', 'question', 'active listening', 'reflection'])\n        data.append((sentence, technique))\n    return pd.DataFrame(data, columns=['sentence', 'technique'])\n\n# Generate mock data\ndf = generate_mock_data()\n\n# Save the data to a CSV file\ndf.to_csv('mock_data.csv', index=False)",
      "outputs": []
    },
    {
      "id": "6ea4b429-a969-447b-837e-f49eefec0d6e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0e8b2856-4cf7-4cb6-bbfb-c10c56cdf4ba"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:02:50.558601+00:00",
          "start_time": "2023-06-21T17:02:44.540735+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install faker",
      "outputs": []
    },
    {
      "id": "a63deaf2-f9d8-4e13-b122-e3484a4d062f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "12562559-0f96-45eb-aa3e-4f8f1c73069b"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:03:22.720306+00:00",
          "start_time": "2023-06-21T17:03:22.022993+00:00"
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\nimport faker\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Initialize Faker library\nfake = faker.Faker()\n\n# Define function to generate mock data\ndef generate_mock_data(n=1000):\n    data = []\n    for _ in range(n):\n        # Generate fake sentence\n        sentence = fake.sentence()\n        # Randomly assign a conversational technique\n        technique = random.choice(['affirmation', 'question', 'active listening', 'reflection'])\n        data.append((sentence, technique))\n    return pd.DataFrame(data, columns=['sentence', 'technique'])\n\n# Generate mock data\ndf = generate_mock_data()\n\n# Save the data to a CSV file\ndf.to_csv('mock_data.csv', index=False)",
      "outputs": []
    },
    {
      "id": "225c07d7-7219-478b-af64-f1366547a9f1",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2a95fac7-b7b8-48d2-816b-cc74f1b3f051"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:03:59.065734+00:00",
          "start_time": "2023-06-21T17:03:58.881851+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the data from CSV file\ndf = pd.read_csv('mock_data.csv')\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['sentence'], df['technique'], test_size=0.2, random_state=42)\n\n# Vectorize the sentences\nvectorizer = CountVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\n# Train the Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Predict on the test set\ny_pred = clf.predict(X_test_vec)\n\n# Print the classification report\nprint(classification_report(y_test, y_pred))",
      "outputs": []
    },
    {
      "id": "07c335e7-da1b-4963-bd7b-cb5f9806912e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "cff1e487-cafb-43a9-bad0-1758aad4dd1b"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:06:15.001398+00:00",
          "start_time": "2023-06-21T17:05:27.790713+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow",
      "outputs": []
    },
    {
      "id": "b2e8bd9b-0085-45f6-a52f-cba58d6ed3ec",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bd5e4105-0cc6-4cd5-8dbd-b37e1a6b6209"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:07:07.692385+00:00",
          "start_time": "2023-06-21T17:07:07.222631+00:00"
        }
      },
      "execution_count": null,
      "source": "# Generate more mock data\ndf1 = generate_mock_data(n=5000)\ndf2 = generate_mock_data(n=5000)\ndf3 = generate_mock_data(n=5000)\ndf4 = generate_mock_data(n=5000)\n\n# Combine all the dataframes\ndf = pd.concat([df1, df2, df3, df4])\n\n# Save the data to a CSV file\ndf.to_csv('large_mock_data.csv', index=False)",
      "outputs": []
    },
    {
      "id": "5617c65b-b80f-4f8a-859e-de82c8b4f42f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1251ab02-9b3e-4f2f-8dd3-ba875c0a23d1"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:08:04.633159+00:00",
          "start_time": "2023-06-21T17:08:01.723143+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data from CSV file\ndf = pd.read_csv('large_mock_data.csv')\n\n# Split the data into sentences and labels\nsentences = df['sentence'].values\nlabels = df['technique'].values\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\npadded_sequences = pad_sequences(sequences, padding='post')\n\n# Encode the labels\nencoder = LabelEncoder()\nencoded_labels = encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)",
      "outputs": []
    },
    {
      "id": "04280aea-8c55-4d9a-8b54-9d515daf83a5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f130305f-e421-4402-b8ad-6013c31e71da"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:09:59.686777+00:00",
          "start_time": "2023-06-21T17:08:51.123724+00:00"
        }
      },
      "execution_count": null,
      "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\n\n# Define the model\nmodel = Sequential([\n    Embedding(10000, 64, input_length=X_train.shape[1]),\n    LSTM(64, return_sequences=False),\n    Dense(4, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test))",
      "outputs": []
    },
    {
      "id": "50be280f-e99e-4c6f-b69a-17a216b398f1",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ce5f0d20-7e04-4f26-8ed0-675148e247f1"
        },
        "ExecuteTime": {
          "end_time": "2023-06-21T17:12:04.354028+00:00",
          "start_time": "2023-06-21T17:12:02.793613+00:00"
        }
      },
      "execution_count": null,
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style of seaborn for our plots\nsns.set()\n\n# Plotting the training and validation loss\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'], color='blue', label='Training Loss')\nplt.plot(history.history['val_loss'], color='red', label='Validation Loss')\nplt.title('Training and Validation Loss over epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plotting the training and validation accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['accuracy'], color='blue', label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], color='red', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy over epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()",
      "outputs": []
    },
    {
      "id": "897364ba-9119-4bc8-9828-096475488ada",
      "cell_type": "markdown",
      "source": "## Conclusion and Analysis\n\nOnce upon a time, we embarked on a journey to create a model that can understand and classify different conversational techniques. It was a journey filled with learning, challenges, and ultimately, success. Here's the story of our journey:\n\n1. **The Beginning - Data Generation**: Our journey began in a world without data. But we knew that to train our model, we needed data. So, we created our own. We used a magical tool called the Faker library to create sentences out of thin air. Each sentence was given a special tag - a conversational technique. And just like that, we had our own dataset to start our journey.\n\n2. **The First Attempt - Naive Bayes Classifier**: With our dataset ready, we started training our first model - a Naive Bayes classifier. It was a simple model, a beginner in the world of machine learning. But as we quickly realized, it was too simple for our complex task. So, we decided to bring in the big guns.\n\n3. **The Big Guns - LSTM Model**: We decided to use a more powerful model - a Long Short-Term Memory (LSTM) model. This model was a type of Recurrent Neural Network (RNN), a model known for its ability to understand sequences, like sentences. It was a perfect fit for our task. So, we trained our LSTM model and watched as it learned from our data.\n\n4. **The Test - Model Evaluation**: After our model was trained, it was time to test its skills. We evaluated our model's performance by looking at its accuracy - the percentage of sentences it classified correctly. And to our delight, our model performed quite well, achieving an accuracy of over 98% on our test data.\n\n5. **The Reflection - Data Visualization**: With our journey nearing its end, we decided to look back and reflect on our model's performance. We created plots that showed how our model's performance improved over time. It was a visual representation of our model's learning process - a testament to its growth and our success.\n\nAnd so, our journey came to an end. We had successfully created a model that could classify sentences based on conversational techniques. But as we looked back, we realized that our journey was not just about the destination, but also about the journey itself. It was about learning, growing, and overcoming challenges. And most importantly, it was about never giving up, no matter how complex the task.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "14f0522d-7108-447a-b3e2-05ef5f399e8e",
      "cell_type": "markdown",
      "source": "## Conclusion and Analysis\n\nOnce upon a time, we embarked on a journey to create a model that can understand and classify different conversational techniques. It was a journey filled with learning, challenges, and ultimately, success. Here's the story of our journey:\n\n1. **The Beginning - Data Generation**: Our journey began in a world without data. But we knew that to train our model, we needed data. So, we created our own. We used a magical tool called the Faker library to create sentences out of thin air. Each sentence was given a special tag - a conversational technique. And just like that, we had our own dataset to start our journey.\n\n2. **The First Attempt - Naive Bayes Classifier**: With our dataset ready, we started training our first model - a Naive Bayes classifier. It was a simple model, a beginner in the world of machine learning. But as we quickly realized, it was too simple for our complex task. So, we decided to bring in the big guns.\n\n3. **The Big Guns - LSTM Model**: We decided to use a more powerful model - a Long Short-Term Memory (LSTM) model. This model was a type of Recurrent Neural Network (RNN), a model known for its ability to understand sequences, like sentences. It was a perfect fit for our task. So, we trained our LSTM model and watched as it learned from our data.\n\n4. **The Test - Model Evaluation**: After our model was trained, it was time to test its skills. We evaluated our model's performance by looking at its accuracy - the percentage of sentences it classified correctly. And to our delight, our model performed quite well, achieving an accuracy of over 98% on our test data.\n\n5. **The Reflection - Data Visualization**: With our journey nearing its end, we decided to look back and reflect on our model's performance. We created plots that showed how our model's performance improved over time. It was a visual representation of our model's learning process - a testament to its growth and our success.\n\nAnd so, our journey came to an end. We had successfully created a model that could classify sentences based on conversational techniques. But as we looked back, we realized that our journey was not just about the destination, but also about the journey itself. It was about learning, growing, and overcoming challenges. And most importantly, it was about never giving up, no matter how complex the task.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "d9f4cc0f-87c8-4970-8043-cd6fa7f8f7f9",
      "cell_type": "markdown",
      "source": "In this cell, we are importing necessary libraries and defining a function to generate mock data. The Faker library is used to generate fake sentences and each sentence is randomly assigned a conversational technique.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "a23c92b7-90a6-41cf-9a7f-d950b09dc1be",
      "cell_type": "markdown",
      "source": "In this cell, we are installing the Faker library which will be used to generate fake sentences for our mock data.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5c7fd35b-0a97-4fb0-80ad-74bbbf6e4c27",
      "cell_type": "markdown",
      "source": "In this cell, we are generating the mock data using the function we defined earlier. The generated data is then saved to a CSV file.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "237f5331-f8d1-40f2-9662-da649b308ae7",
      "cell_type": "markdown",
      "source": "In this cell, we are loading the data from the CSV file and splitting it into training and testing sets. The sentences are then vectorized and a Naive Bayes classifier is trained on the training data. The classifier is then used to predict the conversational techniques of the sentences in the testing set. The performance of the classifier is evaluated by comparing the predicted techniques with the actual techniques.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "37a2a714-dead-4a36-b874-e594700fa288",
      "cell_type": "markdown",
      "source": "In this cell, we are installing the TensorFlow library which will be used to create and train a deep learning model.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "96f68c67-d6b3-409d-8c3b-2787072f3a32",
      "cell_type": "markdown",
      "source": "In this cell, we are installing the Faker library which will be used to generate fake sentences for our mock data.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "10f18d76-8297-4aa1-90f9-e8dec3bae6c9",
      "cell_type": "markdown",
      "source": "In this cell, we are importing necessary libraries and defining a function to generate mock data. The Faker library is used to generate fake sentences and each sentence is randomly assigned a conversational technique.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "84443d3e-c704-431e-a518-c5ef028e0ca9",
      "cell_type": "markdown",
      "source": "In this cell, we are generating the mock data using the function we defined earlier. The generated data is then saved to a CSV file.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}